{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3Bs1cY5Ly0b",
        "outputId": "7fbf6a14-5b77-4003-d25c-c884c7bc7b6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.8)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost optuna lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "r0RfqI0OKpqF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import boxcox\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, \\\n",
        "cross_val_score, cross_validate, StratifiedKFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "import optuna\n",
        "from optuna.importance import get_param_importances, FanovaImportanceEvaluator\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, \\\n",
        "f1_score, roc_auc_score, make_scorer\n",
        "from optuna.pruners import ThresholdPruner\n",
        "import calendar\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, ClusterCentroids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "IwiUgg89Ku6c"
      },
      "outputs": [],
      "source": [
        "updated_columns = ['customer_id', 'purchase_date', 'is_local', 'age', 'gender', 'mobile_model', 'price',\n",
        "                   'is_from_facebook_page', 'is_facebook_page_follower', 'is_returning_customer',\n",
        "                   'awareness_through_marketing']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "w024BycIKwZT"
      },
      "outputs": [],
      "source": [
        "def get_data_frame(apply_price_transformation:bool = False,\n",
        "                   is_returning_customer_dependent_variable:bool = False)->pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Loads data from CSV file into DataFrame and returns the same.\n",
        "    :return: DataFrame holding data from CSV file\n",
        "    \"\"\"\n",
        "    dataset = pd.read_csv('TechCorner_Sales_update.csv')\n",
        "    dataset.columns = updated_columns\n",
        "\n",
        "    # Date Handling\n",
        "    dataset['purchase_date'] = pd.to_datetime(dataset['purchase_date'], errors='raise', dayfirst=True)\n",
        "\n",
        "    # Periodic Features\n",
        "    dataset['day_of_year'] = dataset['purchase_date'].dt.dayofyear\n",
        "\n",
        "    # Check leap year (366 days if leap year, else 365)\n",
        "    dataset['year'] = dataset['purchase_date'].dt.year\n",
        "    dataset['days_in_year'] = dataset['year'].apply(lambda x: 366 if calendar.isleap(x) else 365)\n",
        "\n",
        "    # sine/cosine transformations\n",
        "    dataset['sin_day_of_year'] = np.sin(2*np.pi*dataset['day_of_year']/dataset['days_in_year'])\n",
        "    dataset['cos_day_of_year'] = np.cos(2 * np.pi * dataset['day_of_year'] / dataset['days_in_year'])\n",
        "\n",
        "    # Drop purchase date column\n",
        "    dataset.drop(columns=['customer_id', 'purchase_date', 'day_of_year', 'year', 'days_in_year'], inplace=True)\n",
        "\n",
        "\n",
        "    # map yes/no to 1/0, instead of performing one hot encoding\n",
        "    dataset['is_local'] = dataset['is_local'].map({'Rangamati Sadar':1, 'Inside Rangamati':1, 'Outside Rangamati':0})\n",
        "    dataset['gender'] = dataset['gender'].map({'F':1, 'M':0})\n",
        "    dataset['is_from_facebook_page'] = dataset['is_from_facebook_page'].map({'Yes':1, 'No':0})\n",
        "    dataset['is_facebook_page_follower'] = dataset['is_facebook_page_follower'].map({'Yes':1, 'No':0})\n",
        "    dataset['is_returning_customer'] = dataset['is_returning_customer'].map({'Yes':1, 'No':0})\n",
        "    dataset['awareness_through_marketing'] = dataset['awareness_through_marketing'].map({'Yes':1, 'No':0})\n",
        "\n",
        "    if apply_price_transformation:\n",
        "        dataset['price'], lambda_bc = boxcox(dataset['price'])\n",
        "\n",
        "    last_column = ['is_from_facebook_page']\n",
        "    if is_returning_customer_dependent_variable:\n",
        "        last_column = ['is_returning_customer']\n",
        "\n",
        "    columns_at_start = [\"age\", \"price\", \"mobile_model\"]\n",
        "    re_ordered_columns = (columns_at_start +\n",
        "                          [col for col in dataset.columns if col not in columns_at_start + last_column] +\n",
        "                          last_column)\n",
        "\n",
        "    dataset = dataset[re_ordered_columns]\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "RLHfNIHhK2Lj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3a1c324-b109-4f97-f782-cc8b97a278f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['age', 'price', 'mobile_model', 'is_local', 'gender',\n",
            "       'is_from_facebook_page', 'is_facebook_page_follower',\n",
            "       'awareness_through_marketing', 'sin_day_of_year', 'cos_day_of_year',\n",
            "       'is_returning_customer'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Load data set\n",
        "dataset = get_data_frame(False, True)\n",
        "print(dataset.columns)\n",
        "\n",
        "# # Separate dependent and independent variables\n",
        "X = dataset.iloc[:, :-1]\n",
        "y = dataset.iloc[:, -1]\n",
        "\n",
        "\n",
        "# Split data into Training and Test Set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y)\n",
        "# print(len(y[y == 0]))\n",
        "# print(len(y[y == 1]))\n",
        "\n",
        "\n",
        "# Apply Standard/Robust Scaling/one hot encoding\n",
        "standard_scaling_features = [0] # Age\n",
        "robust_scaling_features = [1] # Price\n",
        "categorical_features = [2] # Mobile Model\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('standardscaler', StandardScaler(), standard_scaling_features)\n",
        "    , ('robustscaler', RobustScaler(), robust_scaling_features)\n",
        "    ,('onehotencoder', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "], remainder='passthrough')\n",
        "\n",
        "pipeline = Pipeline([('preprocessor', preprocessor)])\n",
        "X_train_transformed = pipeline.fit_transform(X_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will first perform hyper paramater tuning (using optuna) to various machine learning models to identify parameter values corresponding to maximum F1 score. Then we will use those parameter values to get corresponding accuracy of these models, so that we will have balance between F1 score and accuracy. As this is imabalanced data set, we used StratifiedKFold validation. You can find commented code to Prune Trials, which you can use in case you want prune trials that are not reachining targeted scores."
      ],
      "metadata": {
        "id": "PmvnHbLJapwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optuna Objective Functions"
      ],
      "metadata": {
        "id": "eff16GQKjV1g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "yyP8uxQX4WbQ"
      },
      "outputs": [],
      "source": [
        "from math import log\n",
        "def xgb_objective(trial):\n",
        "    params = {\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 6),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500, log=True),\n",
        "        'subsample':trial.suggest_float(\"subsample\", 0.6, 1, log=True),\n",
        "        'colsample_bytree':trial.suggest_float(\"colsample_bytree\", 0.6, 1, log=True),\n",
        "        'scale_pos_weight': 6677/2194\n",
        "    }\n",
        "    # Params leading to Max f1 score. Use them to find corresponding accuracy\n",
        "    # params = {\n",
        "    #     \"max_depth\": 2,\n",
        "    #     \"learning_rate\": 0.01219299570440381,\n",
        "    #     \"n_estimators\": 131,\n",
        "    #     'subsample':0.8091079599945468,\n",
        "    #     'colsample_bytree':0.6333120861160056,\n",
        "    #     'scale_pos_weight': 6677/2194\n",
        "    # }\n",
        "\n",
        "    model = XGBClassifier(**params)\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True,random_state=42)\n",
        "    score = cross_val_score(model, X_train_transformed, y_train, cv=cv, scoring=\"f1\").mean()\n",
        "    # Report accuracy at step 1\n",
        "    # trial.report(score, step=1)\n",
        "    # Prune if accuracy is below 0.75\n",
        "    # if trial.should_prune():\n",
        "    #   raise optuna.TrialPruned()\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "id": "cgBhil0GJNp3"
      },
      "outputs": [],
      "source": [
        "def logistic_regression_objective(trial):\n",
        "\n",
        "  tol = trial.suggest_float('tol' , 0.001, 1000)\n",
        "  c = trial.suggest_float('C', 1e-3, 1e3, log=True)\n",
        "  penalty = trial.suggest_categorical('penalty',['l1', 'l2', 'elasticnet'])\n",
        "  solver = trial.suggest_categorical('solver',['lbfgs', 'liblinear', 'newton-cg','saga'])\n",
        "  max_iterations = trial.suggest_int('max_iterations', 100, 1000)\n",
        "  l1_ratio = trial.suggest_float('l1_ratio', 0, 1) if penalty == 'elasticnet' else None\n",
        "  class_weight = trial.suggest_categorical('class_weight',['balanced', None])\n",
        "  # Ensure solver compatability\n",
        "  if penalty == 'l1' and solver not in ['liblinear','saga']:\n",
        "    return float('-inf')\n",
        "  if penalty == 'elasticnet' and solver != 'saga':\n",
        "    return float('-inf')\n",
        "\n",
        "  model =  LogisticRegression(tol=tol, C=c, penalty=penalty, solver=solver, max_iter=max_iterations, l1_ratio = l1_ratio\n",
        "                               ,class_weight=class_weight)\n",
        "  # Params leading to Max f1 score. Use them to find corresponding accuracy\n",
        "  # {'tol': 540.1788332371493, 'C': 22.514489375753627, 'penalty': 'l1', 'solver': 'saga', 'max_iterations': 163,\n",
        "  # 'class_weight': 'balanced'}\n",
        "  # model =  LogisticRegression(C=22.514489375753627, tol=540.1788332371493, solver='saga', penalty='l1', max_iter=163,\n",
        "  #                              class_weight='balanced')\n",
        "  cv = StratifiedKFold(n_splits=5, shuffle=True,random_state=42)\n",
        "  score = cross_val_score(model, X_train_transformed, y_train, cv=cv, scoring='f1').mean()\n",
        "  # Report accuracy at step 1\n",
        "  # trial.report(score, step=1)\n",
        "  # Prune if accuracy is below Threshold\n",
        "  # if trial.should_prune():\n",
        "  #   raise optuna.TrialPruned()\n",
        "  return score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "vd3SlRlpgi5s"
      },
      "outputs": [],
      "source": [
        "def knn_objective(trial):\n",
        "  params = {\n",
        "      'n_neighbors' : trial.suggest_int('n_neighbors', 5, 100),\n",
        "      'weights' : trial.suggest_categorical('weights', ['uniform','distance']),\n",
        "      'algorithm' : trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute']),\n",
        "      'leaf_size' : trial.suggest_int('leaf_size', 10, 300),\n",
        "      'p' : trial.suggest_int('p', 1,2)\n",
        "      }\n",
        "  # Params leading to Max f1 score. Use them to find corresponding accuracy\n",
        "  # params={'n_neighbors': 5, 'weights': 'distance', 'algorithm': 'brute', 'leaf_size': 296, 'p': 1}\n",
        "\n",
        "  model = KNeighborsClassifier(**params, n_jobs=-1)\n",
        "\n",
        "  cv = StratifiedKFold(n_splits=5, shuffle=True,random_state=42)\n",
        "  score = cross_val_score(model, X_train_transformed, y_train, cv=cv, scoring='f1', n_jobs=-1).mean()\n",
        "  # trial.report(score, step=1)\n",
        "\n",
        "  # if trial.should_prune():\n",
        "  #   raise optuna.TrialPruned()\n",
        "  return score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "093oFF0WqLMo"
      },
      "outputs": [],
      "source": [
        "def linear_svc_objective(trial):\n",
        "  penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])\n",
        "  loss = trial.suggest_categorical('loss', ['hinge','squared_hinge'])\n",
        "  dual = trial.suggest_categorical('dual', [False, 'auto'])\n",
        "  tol = trial.suggest_float('tol', 1e-6, 1e-2, log=True)\n",
        "  C = trial.suggest_float('C', 1e-3, 1e3, log=True)\n",
        "  fit_intercept = trial.suggest_categorical('fit_intercept', [True, False])\n",
        "  class_weight = trial.suggest_categorical('class_weight', [None, 'balanced'])\n",
        "  max_iter = trial.suggest_int('max_iter',1000, 10000)\n",
        "\n",
        "  if penalty == 'l1' and loss == 'hinge':\n",
        "    return float('-inf')\n",
        "  if penalty == 'l2' and loss == 'hinge' and dual == False:\n",
        "    return float('-inf')\n",
        "\n",
        "  model = LinearSVC(penalty=penalty, loss=loss, dual=dual, tol=tol,\n",
        "                    C=C, fit_intercept=fit_intercept,\n",
        "                    class_weight=class_weight, max_iter=max_iter)\n",
        "\n",
        "  # Params leading to Max f1 score. Use them to find corresponding accuracy\n",
        "  # params={'penalty': 'l1', 'loss': 'squared_hinge', 'dual': False, 'tol': 1.2930365756581555e-05,\n",
        "  #         'C': 1.0361104265170653, 'fit_intercept': True, 'class_weight': 'balanced', 'max_iter': 6805}\n",
        "  # model = LinearSVC(**params)\n",
        "  cv = StratifiedKFold(n_splits=5, shuffle=True,random_state=42)\n",
        "  score = cross_val_score(model, X_train_transformed, y_train, cv=cv, scoring='f1', n_jobs=-1).mean()\n",
        "  # trial.report(score, step=1)\n",
        "\n",
        "  # if trial.should_prune():\n",
        "  #   raise optuna.TrialPruned()\n",
        "  return score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "e5ZTxpgz1X4T"
      },
      "outputs": [],
      "source": [
        "def svc_objective(trial):\n",
        "\n",
        "  C = trial.suggest_float('C', 1e-3, 1e3, log=True)\n",
        "  kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n",
        "  # Parameters based on kernel Choice\n",
        "  degree = trial.suggest_int('degree', 1, 5) if kernel == 'poly' else 0\n",
        "  gamma = trial.suggest_float('gamma', 1e-4, 1e1, log=True) if kernel in ['poly', 'rbf', 'sigmoid'] else 0\n",
        "  coef0 = trial.suggest_float('coef0', 0, 1) if kernel in ['poly','sigmoid'] else 0\n",
        "  shrinking = trial.suggest_categorical('shrinking', [True, False])\n",
        "  tol = trial.suggest_float('tol', 1e-6, 1e-2, log=True)\n",
        "  class_weight = trial.suggest_categorical('class_weight', [None, 'balanced'])\n",
        "\n",
        "  if kernel == 'poly' and degree is None:\n",
        "    return float('-inf')\n",
        "  if kernel in ['poly', 'sigmoid'] and coef0 is None:\n",
        "    return float('-inf')\n",
        "\n",
        "  model = SVC(C=C, kernel=kernel, degree=degree, gamma = gamma, coef0=coef0,\n",
        "              shrinking=shrinking, tol=tol, class_weight=class_weight, max_iter=2000)\n",
        "  # Params leading to Max f1 score. Use them to find corresponding accuracy\n",
        "  # params={'C': 0.013411234582791877, 'kernel': 'rbf', 'gamma': 1.4470227444863837, 'shrinking': True,\n",
        "  #         'tol': 0.00040054839483054177, 'class_weight': 'balanced'}\n",
        "  # model = SVC(**params)\n",
        "  cv = StratifiedKFold(n_splits=5, shuffle=True,random_state=42)\n",
        "  score = cross_val_score(model, X_train_transformed, y_train, cv=cv, n_jobs=-1, scoring='f1').mean()\n",
        "  # trial.report(score, step=1)\n",
        "\n",
        "  # if trial.should_prune():\n",
        "  #   raise optuna.TrialPruned()\n",
        "\n",
        "  return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "id": "LZBH4h0QiGgX"
      },
      "outputs": [],
      "source": [
        "def randomforest_objective(trial):\n",
        "\n",
        "  params = {\n",
        "    'n_estimators' : trial.suggest_int('n_estimators', 50, 500),\n",
        "    'max_depth' : trial.suggest_int('max_depth', 5, 50),\n",
        "    'min_samples_split' : trial.suggest_int('min_samples_split', 2, 20),\n",
        "    'min_samples_leaf' : trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "    'max_features' : trial.suggest_categorical('max_features', ['sqrt','log2', None]),\n",
        "    'bootstrap' : trial.suggest_categorical('bootstrap', [True, False]),\n",
        "    'class_weight' : trial.suggest_categorical('class_weight', ['balanced','balanced_subsample', None]),\n",
        "    'criterion' : trial.suggest_categorical('criterion', ['gini','entropy','log_loss'])\n",
        "  }\n",
        "  # Params leading to Max f1 score. Use them to find corresponding accuracy\n",
        "  # params={'n_estimators': 465, 'max_depth': 7, 'min_samples_split': 12, 'min_samples_leaf': 4, 'max_features': None,\n",
        "  #         'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'gini'}\n",
        "  model = RandomForestClassifier(**params)\n",
        "  cv = StratifiedKFold(n_splits=5, shuffle=True,random_state=42)\n",
        "  score = cross_val_score(model,X_train_transformed, y_train, cv=cv, n_jobs=-1,scoring='f1').mean()\n",
        "\n",
        "  # trial.report(score, step=1)\n",
        "\n",
        "  # if trial.should_prune():\n",
        "  #   raise optuna.TrialPruned()\n",
        "\n",
        "  return score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "D9IBUVEmcahN"
      },
      "outputs": [],
      "source": [
        "def decission_tree_objective(trial):\n",
        "\n",
        "  params = {\n",
        "      'criterion' : trial.suggest_categorical('criterion', ['gini','entropy']),\n",
        "      'max_depth' : trial.suggest_int('max_depth', 3, 15),\n",
        "      'min_samples_split' : trial.suggest_int('min_samples_split', 2, 50),\n",
        "      'min_samples_leaf' : trial.suggest_int('min_samples_leaf', 1, 50),\n",
        "      'max_features' : trial.suggest_float('max_features', 0.1, 1.0),\n",
        "      'ccp_alpha' : trial.suggest_float('ccp_alpha', 0.001, 0.1),\n",
        "      'class_weight' : trial.suggest_categorical('class_weight', ['balanced', None])\n",
        "  }\n",
        "  # Params leading to Max f1 score. Use them to find corresponding accuracy\n",
        "  # params={'criterion': 'gini', 'max_depth': 12, 'min_samples_split': 5, 'min_samples_leaf': 9,\n",
        "  #         'max_features': 0.45192524410340684, 'ccp_alpha': 0.033735311554373926, 'class_weight': 'balanced'}\n",
        "  model = DecisionTreeClassifier(splitter='best', **params)\n",
        "  cv = StratifiedKFold(n_splits=5, shuffle=True,random_state=42)\n",
        "  score = cross_val_score(model, X_train_transformed, y_train, cv=cv, n_jobs=-1, scoring='f1').mean()\n",
        "  # trial.report(score, step=1)\n",
        "\n",
        "  # if trial.should_prune():\n",
        "  #   raise optuna.TrialPruned()\n",
        "\n",
        "  return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "id": "IOW3anK4xoQi"
      },
      "outputs": [],
      "source": [
        "def lightgbm_objective(trial):\n",
        "\n",
        "  params = {\n",
        "      'num_leaves' : trial.suggest_int('num_leaves', 10, 50, log=True),\n",
        "      'max_depth' : trial.suggest_int('max_depth', 3, 8, log=True),\n",
        "      'min_data_in_leaf' : trial.suggest_int('min_data_in_leaf', 10, 100, log=True),\n",
        "      \"learning_rate\" : trial.suggest_float(\"learning_rate\", 0.001, 0.05, log=True),\n",
        "      'n_estimators' : trial.suggest_int('n_estimators', 100, 1000, log=True),\n",
        "      'max_bin' : trial.suggest_int('max_bin', 32, 128, log=True),\n",
        "      'num_iterations' : trial.suggest_int('num_iterations', 100, 1000, log=True)\n",
        "  }\n",
        "  # Params leading to Max f1 score. Use them to find corresponding accuracy\n",
        "  # params={'num_leaves': 41, 'max_depth': 3, 'min_data_in_leaf': 14, 'learning_rate': 0.049741212348164456,\n",
        "  #         'n_estimators': 252, 'max_bin': 107, 'num_iterations': 130}\n",
        "  model = LGBMClassifier(**params, is_unbalance=True)\n",
        "  cv = StratifiedKFold(n_splits=5, shuffle=True,random_state=42)\n",
        "  score = cross_val_score(model, X_train_transformed, y_train, cv=cv, n_jobs=-1, scoring='f1').mean()\n",
        "  # trial.report(score, step=1)\n",
        "\n",
        "  # if trial.should_prune():\n",
        "  #   raise optuna.TrialPruned()\n",
        "\n",
        "  return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "kymDOZCannW2"
      },
      "outputs": [],
      "source": [
        "def catboost_objective(trial):\n",
        "  params = {\n",
        "      'iterations' : trial.suggest_int('iterations', 100, 1000, log=True),\n",
        "      'depth' : trial.suggest_int('depth', 3, 8, log=True),\n",
        "      'learning_rate' : trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "      'auto_class_weights' : trial.suggest_categorical('auto_class_weights', ['None', 'Balanced', 'SqrtBalanced'])\n",
        "  }\n",
        "  # Params leading to Max f1 score. Use them to find corresponding accuracy\n",
        "  # params={'iterations': 188, 'depth': 4, 'learning_rate': 0.013800184228352417, 'auto_class_weights': 'Balanced'}\n",
        "  model = CatBoostClassifier(**params) #\n",
        "  cv = StratifiedKFold(n_splits=5, shuffle=True,random_state=42)\n",
        "  score = cross_val_score(model, X_train_transformed, y_train, cv=cv, n_jobs=-1, scoring='f1').mean()\n",
        "  # trial.report(score, step=1)\n",
        "\n",
        "  # if trial.should_prune():\n",
        "  #   raise optuna.TrialPruned()\n",
        "\n",
        "  return score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stacking Machine Learning Models"
      ],
      "metadata": {
        "id": "mhzeOfR7jrMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stacking CatBoostClassifier, LGBMClassifier, XGBClassifier together by using LogisticRegression model as meta model(Final Estimator)"
      ],
      "metadata": {
        "id": "V3HIovmtkJMN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "id": "MMHnK7EhoTgo"
      },
      "outputs": [],
      "source": [
        "def stacking_objective(trial):\n",
        "\n",
        "  params_cat_boost={'iterations': 188, 'depth': 4, 'learning_rate': 0.013800184228352417, 'auto_class_weights': 'Balanced'}\n",
        "  model_cat_boost = CatBoostClassifier(**params_cat_boost, random_state=42)\n",
        "\n",
        "  params_lgbm={'num_leaves': 41, 'max_depth': 3, 'min_data_in_leaf': 14, 'learning_rate': 0.049741212348164456,\n",
        "          'n_estimators': 252, 'max_bin': 107, 'num_iterations': 130}\n",
        "  model_lgbm = LGBMClassifier(**params_lgbm, is_unbalance=True, random_state=42)\n",
        "\n",
        "  params_xgb = {\n",
        "        \"max_depth\": 2,\n",
        "        \"learning_rate\": 0.01219299570440381,\n",
        "        \"n_estimators\": 131,\n",
        "        'subsample':0.8091079599945468,\n",
        "        'colsample_bytree':0.6333120861160056,\n",
        "        'scale_pos_weight': 6677/2194\n",
        "    }\n",
        "\n",
        "  model_xgb = XGBClassifier(**params_xgb, random_state=42)\n",
        "\n",
        "  meta_model =  LogisticRegression(C=22.514489375753627, tol=540.1788332371493, solver='saga', penalty='l1', max_iter=163,\n",
        "                               class_weight='balanced', random_state=42)\n",
        "\n",
        "  stacked_model = StackingClassifier(estimators=[('catboost', model_cat_boost), ('lgbm',model_lgbm),\n",
        "                                                 ('xgb',model_xgb)], final_estimator=meta_model)\n",
        "\n",
        "  cv = StratifiedKFold(n_splits=5, shuffle=True,random_state=42)\n",
        "  score = cross_val_score(stacked_model, X_train_transformed, y_train, cv=cv, n_jobs=-1, scoring='f1').mean()\n",
        "\n",
        "  return score\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Probablity Threshold Objective Function"
      ],
      "metadata": {
        "id": "TJQjd6Zjjz-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Varying Probabiliity thresholds between 0.5 to 0.6 to see if it can improve F1 and accuracy metrics"
      ],
      "metadata": {
        "id": "DvFlclT3kgvI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "id": "nvNIQCGa8mAo"
      },
      "outputs": [],
      "source": [
        "def prob_threshold_objective(trial):\n",
        "  threshold = trial.suggest_float('threshold', 0.5, 0.6, log=True)\n",
        "  params_xgb={'max_depth': 6, 'learning_rate': 0.11047765334239668, 'n_estimators': 194, 'subsample': 0.6464190923360026,\n",
        "        'colsample_bytree': 0.7941654326235098,'scale_pos_weight': 6677/2194}\n",
        "\n",
        "  model_xgb = XGBClassifier(**params_xgb, random_state=42)\n",
        "  cv = StratifiedKFold(n_splits=5, shuffle=True,random_state=42)\n",
        "  f1_scores = []\n",
        "\n",
        "  for train_idx, val_idx in cv.split(X_train_transformed, y_train):\n",
        "    X_cv_train, X_cv_val = X_train_transformed[train_idx], X_train_transformed[val_idx]\n",
        "    y_cv_train, y_cv_val = y_train[train_idx], y_train[val_idx]\n",
        "    model_xgb.fit(X_cv_train, y_cv_train)\n",
        "    # Get probability predictions\n",
        "    y_proba = model_xgb.predict_proba(X_cv_val)[:, 1]\n",
        "\n",
        "    # Apply tuned probability threshold\n",
        "    y_predict = (y_proba >= threshold).astype(int)\n",
        "    f1_scores.append(precision_score(y_cv_val, y_predict))\n",
        "\n",
        "  return np.mean(f1_scores)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SMOTE Sampling"
      ],
      "metadata": {
        "id": "NfAtTSvSk0jf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the data set is imbalanced performed smote sampling to increase counts for minority class, there by trying to see if it can improve F1 and accuracy scores"
      ],
      "metadata": {
        "id": "h3ZbLTGNk4ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def smote_objective(trial):\n",
        "  smote_sampling_ratio = trial.suggest_float(\"sampling_strategy\",0.4, 1.0)\n",
        "  # smote_tomek_sampling_ratio = trial.suggest_float(\"sampling_strategy\",0.5, smote_sampling_ratio)\n",
        "  # smote_enn_sampling_ratio = trial.suggest_float(\"sampling_strategy\",0.4, smote_tomek_sampling_ratio)\n",
        "  dataset = get_data_frame(False, True)\n",
        "  # print(dataset.columns)\n",
        "  # Separate dependent variable vector\n",
        "  y = dataset.iloc[:, -1].values\n",
        "  # categorical encoding of mobile_model\n",
        "  dataset = pd.get_dummies(dataset.drop(columns=['is_returning_customer']), drop_first=False, columns=['mobile_model'])\n",
        "  # print(dataset.columns)\n",
        "\n",
        "  # Separate independent variables\n",
        "  X = dataset.iloc[:, :].values\n",
        "\n",
        "\n",
        "  # Split data into Training and Test Set\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y, random_state=0)\n",
        "\n",
        "  # smote = SMOTE(sampling_strategy=smote_sampling_ratio, random_state=0)\n",
        "  # X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "  # smote_tomek = SMOTETomek(sampling_strategy=smote_sampling_ratio, random_state=0)\n",
        "  # X_train, y_train = smote_tomek.fit_resample(X_train, y_train)\n",
        "\n",
        "  smote_enn = SMOTEENN(sampling_strategy=smote_sampling_ratio, random_state=0)\n",
        "  X_train, y_train = smote_enn.fit_resample(X_train, y_train)\n",
        "\n",
        "  # Apply Standard/Robust Scaling\n",
        "  standard_scaling_features = [0] # Age\n",
        "  robust_scaling_features = [1] # Price\n",
        "\n",
        "  preprocessor = ColumnTransformer([\n",
        "      ('standardscaler', StandardScaler(), standard_scaling_features)\n",
        "      , ('robustscaler', RobustScaler(), robust_scaling_features)\n",
        "  ], remainder='passthrough')\n",
        "\n",
        "  pipeline = Pipeline([('preprocessor', preprocessor)])\n",
        "  X_train_transformed = pipeline.fit_transform(X_train)\n",
        "  X_test = pipeline.transform(X_test)\n",
        "\n",
        "  params_lgbm={'num_leaves': 14, 'max_depth': 5, 'min_data_in_leaf': 13, 'learning_rate': 0.012116103805631632,\n",
        "               'n_estimators': 571, 'max_bin': 66, 'num_iterations': 888}\n",
        "  model_lgbm = LGBMClassifier(**params_lgbm, verbose=-1) #94/61\n",
        "  model_lgbm.fit(X_train_transformed, y_train)\n",
        "\n",
        "  y_smote_predict = model_lgbm.predict(X_test)\n",
        "  return f1_score(y_test, y_smote_predict)\n",
        "\n"
      ],
      "metadata": {
        "id": "fMU1S3bpD3_W"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Under Sampling"
      ],
      "metadata": {
        "id": "GAc-bNgZmYjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the data set is imbalanced performed under sampling to decrease counts for majority class, there by trying to see if it can improve F1 and accuracy scores"
      ],
      "metadata": {
        "id": "qKeT5ltVmbiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def under_sampling_objective(trial):\n",
        "  # under_sampling_ratio = trial.suggest_float('sampling_strategy', 0.35, 1)\n",
        "  dataset = get_data_frame(False, True)\n",
        "  # print(dataset.columns)\n",
        "  # Separate dependent variable vector\n",
        "  y = dataset.iloc[:, -1].values\n",
        "  # categorical encoding of mobile_model\n",
        "  dataset = pd.get_dummies(dataset.drop(columns=['is_returning_customer']), drop_first=False, columns=['mobile_model'])\n",
        "  # print(dataset.columns)\n",
        "\n",
        "  # Separate independent variables\n",
        "  X = dataset.iloc[:, :].values\n",
        "\n",
        "  # Split data into Training and Test Set\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y, random_state=0)\n",
        "\n",
        "  # Apply Standard/Robust Scaling\n",
        "  standard_scaling_features = [0] # Age\n",
        "  robust_scaling_features = [1] # Price\n",
        "\n",
        "  preprocessor = ColumnTransformer([\n",
        "      ('standardscaler', StandardScaler(), standard_scaling_features)\n",
        "      , ('robustscaler', RobustScaler(), robust_scaling_features)\n",
        "  ], remainder='passthrough')\n",
        "\n",
        "  pipeline = Pipeline([('preprocessor', preprocessor)])\n",
        "  X_train_transformed = pipeline.fit_transform(X_train)\n",
        "  X_test = pipeline.transform(X_test)\n",
        "\n",
        "  under_sampler = RandomUnderSampler(random_state=0, sampling_strategy=0.9776536095920548)\n",
        "  # under_sampler = ClusterCentroids()\n",
        "  X_train_transformed, y_train = under_sampler.fit_resample(X_train_transformed, y_train)\n",
        "\n",
        "  params_lgbm={'num_leaves': 14, 'max_depth': 5, 'min_data_in_leaf': 13, 'learning_rate': 0.012116103805631632,\n",
        "               'n_estimators': 571, 'max_bin': 66, 'num_iterations': 888}\n",
        "  model_lgbm = LGBMClassifier(**params_lgbm, verbose=-1, random_state=0) #94/61\n",
        "  model_lgbm.fit(X_train_transformed, y_train)\n",
        "\n",
        "  y_predict = model_lgbm.predict(X_test)\n",
        "  return accuracy_score(y_test, y_predict)\n",
        "\n"
      ],
      "metadata": {
        "id": "K_r5NpMwxIWO"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSqnFbQMMNfU",
        "outputId": "49341a3f-c478-43de-a94e-eaafccd3f436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-13 16:45:56,648] A new study created in memory with name: no-name-c2e6f5c0-4271-455f-9173-275428627950\n",
            "[I 2025-06-13 16:45:58,395] Trial 0 finished with value: 0.33425943984768713 and parameters: {'max_depth': 4, 'learning_rate': 0.019626857389982832, 'n_estimators': 71, 'subsample': 0.8831754870335521, 'colsample_bytree': 0.6233775899205083}. Best is trial 0 with value: 0.33425943984768713.\n",
            "[I 2025-06-13 16:45:59,294] Trial 1 finished with value: 0.3382886900148102 and parameters: {'max_depth': 2, 'learning_rate': 0.022962242529698428, 'n_estimators': 225, 'subsample': 0.6980660824266193, 'colsample_bytree': 0.9847893692326589}. Best is trial 1 with value: 0.3382886900148102.\n",
            "[I 2025-06-13 16:46:01,645] Trial 3 finished with value: 0.30392888564145626 and parameters: {'max_depth': 6, 'learning_rate': 0.07628828571592199, 'n_estimators': 60, 'subsample': 0.7210600688318589, 'colsample_bytree': 0.6860444743638795}. Best is trial 1 with value: 0.3382886900148102.\n",
            "[I 2025-06-13 16:46:07,346] Trial 2 finished with value: 0.3152371949405254 and parameters: {'max_depth': 4, 'learning_rate': 0.028353823765117078, 'n_estimators': 393, 'subsample': 0.9175014632826965, 'colsample_bytree': 0.6100826311405793}. Best is trial 1 with value: 0.3382886900148102.\n",
            "[I 2025-06-13 16:46:08,839] Trial 4 finished with value: 0.3189057192915733 and parameters: {'max_depth': 5, 'learning_rate': 0.012541874358092491, 'n_estimators': 187, 'subsample': 0.8474887651007736, 'colsample_bytree': 0.9861574793686199}. Best is trial 1 with value: 0.3382886900148102.\n",
            "[I 2025-06-13 16:46:09,461] Trial 5 finished with value: 0.34869321940882786 and parameters: {'max_depth': 2, 'learning_rate': 0.013204125980131787, 'n_estimators': 115, 'subsample': 0.8081177445838406, 'colsample_bytree': 0.6560219296027356}. Best is trial 5 with value: 0.34869321940882786.\n",
            "[I 2025-06-13 16:46:10,041] Trial 6 finished with value: 0.34011983510792865 and parameters: {'max_depth': 2, 'learning_rate': 0.05269571397014998, 'n_estimators': 53, 'subsample': 0.785789162888804, 'colsample_bytree': 0.9942199939583446}. Best is trial 5 with value: 0.34869321940882786.\n",
            "[I 2025-06-13 16:46:11,826] Trial 7 finished with value: 0.2891244533742398 and parameters: {'max_depth': 4, 'learning_rate': 0.21559690076915583, 'n_estimators': 110, 'subsample': 0.901586080988307, 'colsample_bytree': 0.8331940821667347}. Best is trial 5 with value: 0.34869321940882786.\n",
            "[I 2025-06-13 16:46:14,285] Trial 9 finished with value: 0.28905373977955795 and parameters: {'max_depth': 4, 'learning_rate': 0.25110026124793, 'n_estimators': 123, 'subsample': 0.668217084808604, 'colsample_bytree': 0.9543718177821785}. Best is trial 5 with value: 0.34869321940882786.\n",
            "[I 2025-06-13 16:46:14,411] Trial 8 finished with value: 0.3048519837118328 and parameters: {'max_depth': 3, 'learning_rate': 0.18944965125352714, 'n_estimators': 304, 'subsample': 0.7194322795224185, 'colsample_bytree': 0.7140530023427822}. Best is trial 5 with value: 0.34869321940882786.\n",
            "[I 2025-06-13 16:46:15,342] Trial 10 finished with value: 0.34124804510387763 and parameters: {'max_depth': 2, 'learning_rate': 0.06203717790855363, 'n_estimators': 69, 'subsample': 0.8759946226153209, 'colsample_bytree': 0.6449566311491822}. Best is trial 5 with value: 0.34869321940882786.\n",
            "[I 2025-06-13 16:46:15,813] Trial 11 finished with value: 0.3500314257548817 and parameters: {'max_depth': 2, 'learning_rate': 0.011070903070572822, 'n_estimators': 94, 'subsample': 0.9953426321206469, 'colsample_bytree': 0.7964218590671913}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:17,195] Trial 12 finished with value: 0.3420678503756339 and parameters: {'max_depth': 2, 'learning_rate': 0.08211897506104626, 'n_estimators': 91, 'subsample': 0.8076368578063773, 'colsample_bytree': 0.6826552243534179}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:19,217] Trial 13 finished with value: 0.33674511887061437 and parameters: {'max_depth': 3, 'learning_rate': 0.011036515647603398, 'n_estimators': 107, 'subsample': 0.6084724599772052, 'colsample_bytree': 0.7998753787488154}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:20,264] Trial 14 finished with value: 0.34410379944547237 and parameters: {'max_depth': 3, 'learning_rate': 0.010704545746992162, 'n_estimators': 149, 'subsample': 0.9911226057293088, 'colsample_bytree': 0.7988273207379438}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:21,781] Trial 15 finished with value: 0.34194098775942866 and parameters: {'max_depth': 3, 'learning_rate': 0.015884103468993157, 'n_estimators': 150, 'subsample': 0.9849485392648156, 'colsample_bytree': 0.8792461721527867}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:22,136] Trial 16 finished with value: 0.34355261824385674 and parameters: {'max_depth': 3, 'learning_rate': 0.03509421167422022, 'n_estimators': 83, 'subsample': 0.9979110340856935, 'colsample_bytree': 0.8740383535153045}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:23,400] Trial 17 finished with value: 0.34725282714329 and parameters: {'max_depth': 2, 'learning_rate': 0.03309299904000372, 'n_estimators': 85, 'subsample': 0.9483001746525271, 'colsample_bytree': 0.7395103057365733}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:26,598] Trial 19 finished with value: 0.2857049697514604 and parameters: {'max_depth': 5, 'learning_rate': 0.13931452928146323, 'n_estimators': 128, 'subsample': 0.8221342277495144, 'colsample_bytree': 0.7459284464597172}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:27,575] Trial 18 finished with value: 0.30008191028115627 and parameters: {'max_depth': 5, 'learning_rate': 0.04068579051583824, 'n_estimators': 227, 'subsample': 0.8322857570501991, 'colsample_bytree': 0.7307750253472316}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:30,427] Trial 20 finished with value: 0.3373360771668496 and parameters: {'max_depth': 2, 'learning_rate': 0.015993838766535064, 'n_estimators': 215, 'subsample': 0.7637764816197523, 'colsample_bytree': 0.667313457733891}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:31,048] Trial 21 finished with value: 0.33688166419187937 and parameters: {'max_depth': 2, 'learning_rate': 0.015629735696607548, 'n_estimators': 175, 'subsample': 0.7477574174631854, 'colsample_bytree': 0.6739192032008137}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:32,742] Trial 22 finished with value: 0.34783764890995444 and parameters: {'max_depth': 2, 'learning_rate': 0.024515081906687684, 'n_estimators': 87, 'subsample': 0.9438728341105415, 'colsample_bytree': 0.7619768267929848}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:33,081] Trial 23 finished with value: 0.348341972387756 and parameters: {'max_depth': 2, 'learning_rate': 0.02510213531120982, 'n_estimators': 90, 'subsample': 0.9464259044183594, 'colsample_bytree': 0.7653907624946076}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:34,559] Trial 24 finished with value: 0.341869992234875 and parameters: {'max_depth': 3, 'learning_rate': 0.02186649794590846, 'n_estimators': 88, 'subsample': 0.941705532822796, 'colsample_bytree': 0.8563239830181612}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:35,064] Trial 25 finished with value: 0.345693492833295 and parameters: {'max_depth': 3, 'learning_rate': 0.01853235861922626, 'n_estimators': 102, 'subsample': 0.9393783636496917, 'colsample_bytree': 0.8507478055840673}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:36,362] Trial 26 finished with value: 0.34312790069826776 and parameters: {'max_depth': 3, 'learning_rate': 0.01009215088950546, 'n_estimators': 109, 'subsample': 0.8605158936862882, 'colsample_bytree': 0.9144989998546309}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:36,814] Trial 27 finished with value: 0.3446468517748077 and parameters: {'max_depth': 2, 'learning_rate': 0.013379803025874833, 'n_estimators': 128, 'subsample': 0.8577561942179251, 'colsample_bytree': 0.9148793759794326}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:37,225] Trial 28 finished with value: 0.3433537871534515 and parameters: {'max_depth': 2, 'learning_rate': 0.014275619674353167, 'n_estimators': 72, 'subsample': 0.6231060903363905, 'colsample_bytree': 0.8080223879951286}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:38,168] Trial 29 finished with value: 0.30877012647603663 and parameters: {'max_depth': 6, 'learning_rate': 0.014969736385872507, 'n_estimators': 69, 'subsample': 0.6449027840269351, 'colsample_bytree': 0.8060232358999541}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:38,565] Trial 30 finished with value: 0.3143698652338406 and parameters: {'max_depth': 6, 'learning_rate': 0.01859508329335378, 'n_estimators': 70, 'subsample': 0.8895926998219289, 'colsample_bytree': 0.6359991531121199}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:38,792] Trial 31 finished with value: 0.34480781925435433 and parameters: {'max_depth': 2, 'learning_rate': 0.01881714397861582, 'n_estimators': 55, 'subsample': 0.8958118084637404, 'colsample_bytree': 0.6376523351681881}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:39,417] Trial 32 finished with value: 0.3419284959651885 and parameters: {'max_depth': 2, 'learning_rate': 0.027648683873022134, 'n_estimators': 95, 'subsample': 0.9627959817611288, 'colsample_bytree': 0.7640154011675965}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:39,642] Trial 33 finished with value: 0.3399741873780828 and parameters: {'max_depth': 2, 'learning_rate': 0.02679605745305333, 'n_estimators': 93, 'subsample': 0.9670168742535763, 'colsample_bytree': 0.7054236578631539}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:40,183] Trial 34 finished with value: 0.34863995501822076 and parameters: {'max_depth': 2, 'learning_rate': 0.024776844977076662, 'n_estimators': 77, 'subsample': 0.9245112331720885, 'colsample_bytree': 0.7715617646800156}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:40,403] Trial 35 finished with value: 0.3457867361942941 and parameters: {'max_depth': 2, 'learning_rate': 0.04260455741094202, 'n_estimators': 80, 'subsample': 0.9265209435784421, 'colsample_bytree': 0.600860399128032}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:40,928] Trial 36 finished with value: 0.3450034411348821 and parameters: {'max_depth': 3, 'learning_rate': 0.04816582293737705, 'n_estimators': 62, 'subsample': 0.9243399100106814, 'colsample_bytree': 0.782835141020937}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:42,465] Trial 38 finished with value: 0.33139957537587555 and parameters: {'max_depth': 4, 'learning_rate': 0.021011945521504025, 'n_estimators': 137, 'subsample': 0.9121567266943611, 'colsample_bytree': 0.8288391787942115}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:43,318] Trial 39 finished with value: 0.34489403433568444 and parameters: {'max_depth': 2, 'learning_rate': 0.0122861311190067, 'n_estimators': 59, 'subsample': 0.7976626825434998, 'colsample_bytree': 0.7108061536296587}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:45,857] Trial 40 finished with value: 0.3198278099498884 and parameters: {'max_depth': 4, 'learning_rate': 0.036310605379204566, 'n_estimators': 122, 'subsample': 0.697765627175775, 'colsample_bytree': 0.7755254250378268}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:46,011] Trial 37 finished with value: 0.3340643277674564 and parameters: {'max_depth': 3, 'learning_rate': 0.012258954697097145, 'n_estimators': 494, 'subsample': 0.9141346960283345, 'colsample_bytree': 0.7806406650496207}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:46,714] Trial 42 finished with value: 0.3480460649973864 and parameters: {'max_depth': 2, 'learning_rate': 0.024711121722442223, 'n_estimators': 77, 'subsample': 0.9562239076759558, 'colsample_bytree': 0.7510003167818164}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:47,403] Trial 43 finished with value: 0.3437169476938968 and parameters: {'max_depth': 2, 'learning_rate': 0.030901186999995705, 'n_estimators': 74, 'subsample': 0.9725193835178516, 'colsample_bytree': 0.7247965361674438}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:47,950] Trial 44 finished with value: 0.33650602483918146 and parameters: {'max_depth': 2, 'learning_rate': 0.09146509908965601, 'n_estimators': 50, 'subsample': 0.8761455775391173, 'colsample_bytree': 0.7499076915488487}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:48,626] Trial 41 finished with value: 0.34111985791921157 and parameters: {'max_depth': 2, 'learning_rate': 0.01183325349030587, 'n_estimators': 387, 'subsample': 0.8725099934198035, 'colsample_bytree': 0.6180623779751249}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:48,854] Trial 45 finished with value: 0.33601780126896497 and parameters: {'max_depth': 2, 'learning_rate': 0.06403090084970746, 'n_estimators': 104, 'subsample': 0.9627865092414697, 'colsample_bytree': 0.8176817285156234}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:49,572] Trial 46 finished with value: 0.3369652009523135 and parameters: {'max_depth': 2, 'learning_rate': 0.061565067132963704, 'n_estimators': 114, 'subsample': 0.956934600239926, 'colsample_bytree': 0.6966428121182161}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:49,628] Trial 47 finished with value: 0.33566861937082526 and parameters: {'max_depth': 3, 'learning_rate': 0.02482024458909285, 'n_estimators': 64, 'subsample': 0.8422574357497771, 'colsample_bytree': 0.6994318557375684}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:50,305] Trial 48 finished with value: 0.3478262318129662 and parameters: {'max_depth': 3, 'learning_rate': 0.025584551264305803, 'n_estimators': 62, 'subsample': 0.9974659218833403, 'colsample_bytree': 0.6591079002910633}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:50,344] Trial 49 finished with value: 0.3458660197960345 and parameters: {'max_depth': 2, 'learning_rate': 0.017259623855577354, 'n_estimators': 79, 'subsample': 0.999978768455737, 'colsample_bytree': 0.6605562191104883}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:51,182] Trial 51 finished with value: 0.3452387000063151 and parameters: {'max_depth': 2, 'learning_rate': 0.020693185454689008, 'n_estimators': 97, 'subsample': 0.7328856585878218, 'colsample_bytree': 0.7591811194969034}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:51,629] Trial 50 finished with value: 0.33265657528569803 and parameters: {'max_depth': 5, 'learning_rate': 0.021912948972604242, 'n_estimators': 79, 'subsample': 0.9788000682558742, 'colsample_bytree': 0.7906890910620189}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:51,920] Trial 52 finished with value: 0.34784180719442487 and parameters: {'max_depth': 2, 'learning_rate': 0.022871050050996572, 'n_estimators': 76, 'subsample': 0.9379873978998365, 'colsample_bytree': 0.7937710705139734}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:52,441] Trial 53 finished with value: 0.3491955291906813 and parameters: {'max_depth': 2, 'learning_rate': 0.030670725090804388, 'n_estimators': 89, 'subsample': 0.9370571445504642, 'colsample_bytree': 0.7338772752879504}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:52,912] Trial 54 finished with value: 0.3431158100519344 and parameters: {'max_depth': 2, 'learning_rate': 0.03063666345995223, 'n_estimators': 116, 'subsample': 0.9014693956580838, 'colsample_bytree': 0.7261775936492421}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:53,416] Trial 55 finished with value: 0.34060347772310795 and parameters: {'max_depth': 2, 'learning_rate': 0.03003875998729263, 'n_estimators': 116, 'subsample': 0.9024666792493466, 'colsample_bytree': 0.7255540297308121}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:53,784] Trial 56 finished with value: 0.3426499400445564 and parameters: {'max_depth': 2, 'learning_rate': 0.03963670861278858, 'n_estimators': 100, 'subsample': 0.91508806199345, 'colsample_bytree': 0.7468739593211418}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:54,731] Trial 57 finished with value: 0.3363593713159042 and parameters: {'max_depth': 2, 'learning_rate': 0.039397035182006226, 'n_estimators': 170, 'subsample': 0.9276581692289979, 'colsample_bytree': 0.7385921753329979}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:55,127] Trial 58 finished with value: 0.33005325293455057 and parameters: {'max_depth': 3, 'learning_rate': 0.04976045684815877, 'n_estimators': 143, 'subsample': 0.770492192263543, 'colsample_bytree': 0.6859078363560609}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:56,321] Trial 59 finished with value: 0.33765844993530675 and parameters: {'max_depth': 3, 'learning_rate': 0.017281100328751377, 'n_estimators': 135, 'subsample': 0.8138946311594081, 'colsample_bytree': 0.6814143355828842}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:56,331] Trial 60 finished with value: 0.3404044385542349 and parameters: {'max_depth': 3, 'learning_rate': 0.013847662902749046, 'n_estimators': 88, 'subsample': 0.8134856804244951, 'colsample_bytree': 0.7658541184269301}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:57,602] Trial 62 finished with value: 0.3447097586590985 and parameters: {'max_depth': 2, 'learning_rate': 0.02462323451942322, 'n_estimators': 79, 'subsample': 0.948055175059389, 'colsample_bytree': 0.8250323941815797}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:57,808] Trial 61 finished with value: 0.3427611165979627 and parameters: {'max_depth': 2, 'learning_rate': 0.024365291320139426, 'n_estimators': 77, 'subsample': 0.9416959292347123, 'colsample_bytree': 0.8353308665198614}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:58,969] Trial 63 finished with value: 0.3438521499230342 and parameters: {'max_depth': 2, 'learning_rate': 0.03429002156223699, 'n_estimators': 86, 'subsample': 0.985034450156052, 'colsample_bytree': 0.8468952172574669}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:58,983] Trial 64 finished with value: 0.3258059623856565 and parameters: {'max_depth': 2, 'learning_rate': 0.29500258270213653, 'n_estimators': 85, 'subsample': 0.980356471264622, 'colsample_bytree': 0.794291131423801}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:59,669] Trial 66 finished with value: 0.34909526360663456 and parameters: {'max_depth': 2, 'learning_rate': 0.010206640247652391, 'n_estimators': 67, 'subsample': 0.9324558560969936, 'colsample_bytree': 0.813286762928498}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:46:59,728] Trial 65 finished with value: 0.34927884154919653 and parameters: {'max_depth': 2, 'learning_rate': 0.010145408155228752, 'n_estimators': 70, 'subsample': 0.9341613062395215, 'colsample_bytree': 0.7876894887521377}. Best is trial 11 with value: 0.3500314257548817.\n",
            "[I 2025-06-13 16:47:00,384] Trial 67 finished with value: 0.35087679969666624 and parameters: {'max_depth': 2, 'learning_rate': 0.010913512999800241, 'n_estimators': 68, 'subsample': 0.699301115010883, 'colsample_bytree': 0.8741344662285595}. Best is trial 67 with value: 0.35087679969666624.\n",
            "[I 2025-06-13 16:47:00,449] Trial 68 finished with value: 0.3469287850163548 and parameters: {'max_depth': 2, 'learning_rate': 0.01025292704668107, 'n_estimators': 67, 'subsample': 0.8834403204013946, 'colsample_bytree': 0.8677977092576536}. Best is trial 67 with value: 0.35087679969666624.\n",
            "[I 2025-06-13 16:47:01,068] Trial 70 finished with value: 0.3434854910910875 and parameters: {'max_depth': 2, 'learning_rate': 0.011116228601912553, 'n_estimators': 55, 'subsample': 0.6672846053042603, 'colsample_bytree': 0.9196945576407399}. Best is trial 67 with value: 0.35087679969666624.\n",
            "[I 2025-06-13 16:47:01,084] Trial 69 finished with value: 0.3492445072606312 and parameters: {'max_depth': 2, 'learning_rate': 0.010692598979014402, 'n_estimators': 66, 'subsample': 0.669429326952495, 'colsample_bytree': 0.8821650277003158}. Best is trial 67 with value: 0.35087679969666624.\n",
            "[I 2025-06-13 16:47:01,709] Trial 72 finished with value: 0.3465255258981889 and parameters: {'max_depth': 2, 'learning_rate': 0.013423326344699399, 'n_estimators': 57, 'subsample': 0.6684550472646754, 'colsample_bytree': 0.895606288613567}. Best is trial 67 with value: 0.35087679969666624.\n",
            "[I 2025-06-13 16:47:01,762] Trial 71 finished with value: 0.3504164549099854 and parameters: {'max_depth': 2, 'learning_rate': 0.01313864056188744, 'n_estimators': 59, 'subsample': 0.6951613750869319, 'colsample_bytree': 0.8943014158755586}. Best is trial 67 with value: 0.35087679969666624.\n",
            "[I 2025-06-13 16:47:02,434] Trial 73 finished with value: 0.3505873463692206 and parameters: {'max_depth': 2, 'learning_rate': 0.011077729188042394, 'n_estimators': 65, 'subsample': 0.6987219789689554, 'colsample_bytree': 0.8989777834181445}. Best is trial 67 with value: 0.35087679969666624.\n",
            "[I 2025-06-13 16:47:02,482] Trial 74 finished with value: 0.3518444883782418 and parameters: {'max_depth': 2, 'learning_rate': 0.011235180999369404, 'n_estimators': 66, 'subsample': 0.7011513531127265, 'colsample_bytree': 0.9587237777205019}. Best is trial 74 with value: 0.3518444883782418.\n",
            "[I 2025-06-13 16:47:03,054] Trial 75 finished with value: 0.34789479463454853 and parameters: {'max_depth': 2, 'learning_rate': 0.011521835019825406, 'n_estimators': 50, 'subsample': 0.708396520035528, 'colsample_bytree': 0.9346835683789629}. Best is trial 74 with value: 0.3518444883782418.\n",
            "[I 2025-06-13 16:47:03,203] Trial 76 finished with value: 0.352366778207123 and parameters: {'max_depth': 2, 'learning_rate': 0.01157100099273626, 'n_estimators': 66, 'subsample': 0.6948883018253642, 'colsample_bytree': 0.9716568646221048}. Best is trial 76 with value: 0.352366778207123.\n",
            "[I 2025-06-13 16:47:03,745] Trial 77 finished with value: 0.3526662983035504 and parameters: {'max_depth': 2, 'learning_rate': 0.013044974229728048, 'n_estimators': 60, 'subsample': 0.6928646610263699, 'colsample_bytree': 0.9707354700431787}. Best is trial 77 with value: 0.3526662983035504.\n",
            "[I 2025-06-13 16:47:03,900] Trial 78 finished with value: 0.3491892551505905 and parameters: {'max_depth': 2, 'learning_rate': 0.012956082527166193, 'n_estimators': 65, 'subsample': 0.6755636001821006, 'colsample_bytree': 0.9692213045772609}. Best is trial 77 with value: 0.3526662983035504.\n",
            "[I 2025-06-13 16:47:04,452] Trial 79 finished with value: 0.35113109730598935 and parameters: {'max_depth': 2, 'learning_rate': 0.015022384191322442, 'n_estimators': 60, 'subsample': 0.685812247536577, 'colsample_bytree': 0.9745893153781793}. Best is trial 77 with value: 0.3526662983035504.\n",
            "[I 2025-06-13 16:47:04,700] Trial 80 finished with value: 0.34415867474825246 and parameters: {'max_depth': 3, 'learning_rate': 0.0161227873380338, 'n_estimators': 60, 'subsample': 0.6884668500598925, 'colsample_bytree': 0.9909438399803698}. Best is trial 77 with value: 0.3526662983035504.\n",
            "[I 2025-06-13 16:47:05,109] Trial 81 finished with value: 0.34816763765658465 and parameters: {'max_depth': 2, 'learning_rate': 0.01459324543454646, 'n_estimators': 59, 'subsample': 0.6858339722803211, 'colsample_bytree': 0.9999306292209961}. Best is trial 77 with value: 0.3526662983035504.\n",
            "[I 2025-06-13 16:47:05,346] Trial 82 finished with value: 0.34863128275311994 and parameters: {'max_depth': 2, 'learning_rate': 0.014308969934392188, 'n_estimators': 54, 'subsample': 0.7223599359699059, 'colsample_bytree': 0.9703933833227004}. Best is trial 77 with value: 0.3526662983035504.\n",
            "[I 2025-06-13 16:47:05,746] Trial 83 finished with value: 0.3471909287783689 and parameters: {'max_depth': 2, 'learning_rate': 0.012911204755547839, 'n_estimators': 54, 'subsample': 0.6537101048002889, 'colsample_bytree': 0.9696678192793693}. Best is trial 77 with value: 0.3526662983035504.\n",
            "[I 2025-06-13 16:47:06,359] Trial 84 finished with value: 0.33361577957898036 and parameters: {'max_depth': 4, 'learning_rate': 0.012155420306988463, 'n_estimators': 70, 'subsample': 0.6545784466370417, 'colsample_bytree': 0.944680237195286}. Best is trial 77 with value: 0.3526662983035504.\n",
            "[I 2025-06-13 16:47:06,506] Trial 85 finished with value: 0.35223084636408514 and parameters: {'max_depth': 2, 'learning_rate': 0.011679687594060935, 'n_estimators': 71, 'subsample': 0.7069974259157724, 'colsample_bytree': 0.9428396459800082}. Best is trial 77 with value: 0.3526662983035504.\n",
            "[I 2025-06-13 16:47:07,037] Trial 86 finished with value: 0.3499548540160078 and parameters: {'max_depth': 2, 'learning_rate': 0.011384384993841675, 'n_estimators': 62, 'subsample': 0.7053538642016313, 'colsample_bytree': 0.9557441340827854}. Best is trial 77 with value: 0.3526662983035504.\n",
            "[I 2025-06-13 16:47:07,175] Trial 87 finished with value: 0.3461825177199839 and parameters: {'max_depth': 2, 'learning_rate': 0.011331199207086416, 'n_estimators': 62, 'subsample': 0.7042893421646502, 'colsample_bytree': 0.9567549255594299}. Best is trial 77 with value: 0.3526662983035504.\n",
            "[I 2025-06-13 16:47:07,667] Trial 88 finished with value: 0.3490739234664435 and parameters: {'max_depth': 2, 'learning_rate': 0.01695470691240681, 'n_estimators': 50, 'subsample': 0.7475120620493033, 'colsample_bytree': 0.901826836526742}. Best is trial 77 with value: 0.3526662983035504.\n",
            "[I 2025-06-13 16:47:08,207] Trial 89 finished with value: 0.33487194139928667 and parameters: {'max_depth': 5, 'learning_rate': 0.015438694915560498, 'n_estimators': 57, 'subsample': 0.7235737732200196, 'colsample_bytree': 0.9008740493348326}. Best is trial 77 with value: 0.3526662983035504.\n",
            "[I 2025-06-13 16:47:08,286] Trial 90 finished with value: 0.35293929677112384 and parameters: {'max_depth': 2, 'learning_rate': 0.015338655903040864, 'n_estimators': 53, 'subsample': 0.7260901466089422, 'colsample_bytree': 0.9330173253181531}. Best is trial 90 with value: 0.35293929677112384.\n",
            "[I 2025-06-13 16:47:09,242] Trial 91 finished with value: 0.3473996104529894 and parameters: {'max_depth': 2, 'learning_rate': 0.01279324228966349, 'n_estimators': 74, 'subsample': 0.6901948430778082, 'colsample_bytree': 0.9328220349863704}. Best is trial 90 with value: 0.35293929677112384.\n",
            "[I 2025-06-13 16:47:09,246] Trial 92 finished with value: 0.3517393997751097 and parameters: {'max_depth': 2, 'learning_rate': 0.01284249929508928, 'n_estimators': 73, 'subsample': 0.6918490637412206, 'colsample_bytree': 0.9300282530168646}. Best is trial 90 with value: 0.35293929677112384.\n",
            "[I 2025-06-13 16:47:10,528] Trial 93 finished with value: 0.3453883571955454 and parameters: {'max_depth': 2, 'learning_rate': 0.01406026321459317, 'n_estimators': 58, 'subsample': 0.6813345469812968, 'colsample_bytree': 0.980953533401436}. Best is trial 90 with value: 0.35293929677112384.\n",
            "[I 2025-06-13 16:47:10,629] Trial 94 finished with value: 0.3455352510119595 and parameters: {'max_depth': 2, 'learning_rate': 0.013875397302993095, 'n_estimators': 72, 'subsample': 0.7142528175379184, 'colsample_bytree': 0.9787377965458813}. Best is trial 90 with value: 0.35293929677112384.\n",
            "[I 2025-06-13 16:47:11,432] Trial 95 finished with value: 0.3500911161449946 and parameters: {'max_depth': 2, 'learning_rate': 0.012303281373217526, 'n_estimators': 52, 'subsample': 0.7379615728586925, 'colsample_bytree': 0.9257908818779789}. Best is trial 90 with value: 0.35293929677112384.\n",
            "[I 2025-06-13 16:47:11,808] Trial 96 finished with value: 0.35008151485921535 and parameters: {'max_depth': 2, 'learning_rate': 0.019745967789194876, 'n_estimators': 53, 'subsample': 0.7353121020237978, 'colsample_bytree': 0.9427151956144965}. Best is trial 90 with value: 0.35293929677112384.\n",
            "[I 2025-06-13 16:47:12,379] Trial 97 finished with value: 0.35270481102543894 and parameters: {'max_depth': 2, 'learning_rate': 0.01937354284451463, 'n_estimators': 64, 'subsample': 0.6967215836157559, 'colsample_bytree': 0.946913019049204}. Best is trial 90 with value: 0.35293929677112384.\n",
            "[I 2025-06-13 16:47:12,492] Trial 98 finished with value: 0.3512324755337763 and parameters: {'max_depth': 2, 'learning_rate': 0.016108383125712056, 'n_estimators': 64, 'subsample': 0.6936843370346133, 'colsample_bytree': 0.9474416603644704}. Best is trial 90 with value: 0.35293929677112384.\n",
            "[I 2025-06-13 16:47:12,790] Trial 99 finished with value: 0.351103741536484 and parameters: {'max_depth': 2, 'learning_rate': 0.01537842165449123, 'n_estimators': 63, 'subsample': 0.7123137883562255, 'colsample_bytree': 0.9104834230295871}. Best is trial 90 with value: 0.35293929677112384.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FrozenTrial(number=90, state=1, values=[0.35293929677112384], datetime_start=datetime.datetime(2025, 6, 13, 16, 47, 7, 672542), datetime_complete=datetime.datetime(2025, 6, 13, 16, 47, 8, 286190), params={'max_depth': 2, 'learning_rate': 0.015338655903040864, 'n_estimators': 53, 'subsample': 0.7260901466089422, 'colsample_bytree': 0.9330173253181531}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'max_depth': IntDistribution(high=6, log=False, low=2, step=1), 'learning_rate': FloatDistribution(high=0.3, log=True, low=0.01, step=None), 'n_estimators': IntDistribution(high=500, log=True, low=50, step=1), 'subsample': FloatDistribution(high=1.0, log=True, low=0.6, step=None), 'colsample_bytree': FloatDistribution(high=1.0, log=True, low=0.6, step=None)}, trial_id=90, value=None)\n"
          ]
        }
      ],
      "source": [
        "study = optuna.create_study(directions=['maximize']) #, pruner=ThresholdPruner(lower=0.65)\n",
        "study.optimize(xgb_objective, n_trials=100, n_jobs=-1)\n",
        "print(study.best_trial)\n",
        "# importance = get_param_importances(study,evaluator=FanovaImportanceEvaluator())\n",
        "# print(importance)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}